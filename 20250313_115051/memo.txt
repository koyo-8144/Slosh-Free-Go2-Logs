Default learning rate update based on KL divergence

"tracking_lin_vel": 1.0,
"tracking_ang_vel": 0.5,
"lin_vel_z": -2.0,
"ang_vel_xy": -0.05,
"base_height": -30.0,
"orientation": -0.1,
"collision": -1.0,
"dof_pos_limits": -10.0,
"torques": -1e-5,
"dof_acc": -2.5e-7,
"action_rate": -0.01,


"num_commands": 3,
"num_obs": 53,
"num_privileged_obs": 56,
"only_positive_rewards": True,
"linvel_update_freq": 50,


Result:
Learning rate is gradually decreasing and mean reward is over the last time when leanring rate dropped at very begginig.
This is what happened in isaaclab but the surrogate loss never get close to zero, need to look into it.


Plan:


Learning rate update from KL -> 1.05

Try feet_airtime reward or contact reward + sin,cos as observations